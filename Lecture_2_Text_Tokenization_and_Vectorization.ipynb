{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/Gen-AI/blob/main/Lecture_2_Text_Tokenization_and_Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDqRHsHmYH_A"
      },
      "source": [
        "# Text Tokenziation and Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgoqZrZLYPlS"
      },
      "source": [
        "Text tokenization is a crucial step in NLP that involves reformatting a piece of text into smaller units called “tokens.” These tokens serve as the building blocks for text vectorization, which converts text into numerical representations (vectors) that machine learning models can work with. A good place to loop up popular tokenizors is https://tiktokenizer.vercel.app/. It is important to know that this tokenization process is not standarized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3LXiETfGIY"
      },
      "source": [
        "# Using Pretrained, Third-Party Vectorizors\n",
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58V-1VqHbQpI"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print word vectors (embeddings) for a specific word\n",
        "print(wv['pizza'])  # Vector for the word 'word2vec'"
      ],
      "metadata": {
        "id": "iAhYXxMx_RKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up. Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through Principal Components Analysis (PCA). Google also did a good job visualizting it in 3D https://projector.tensorflow.org/."
      ],
      "metadata": {
        "id": "QmuEpk_Q3JNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jpuGamMbQpJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select some words to visualize\n",
        "words = ['king', 'queen', 'man', 'woman', 'apple', 'banana', 'car', 'train', 'computer']\n",
        "\n",
        "# Get the word vectors for these words\n",
        "word_vectors = [wv[word] for word in words]\n",
        "\n",
        "# Perform PCA to reduce dimensionality to 2D\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_2d = pca.fit_transform(word_vectors)\n",
        "\n",
        "# Plot the words in 2D space\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o')\n",
        "\n",
        "# Annotate each point with the corresponding word\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11hDkconbQpJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF2wzeFXnwTs"
      },
      "source": [
        "Beyond 3D, it is difficult to visualize the word embedings but we can still measure how close the words are by measuring the consine similarities. There are many ways to calculate the cosine similarity between words and we will use the sklearn function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PzpqlNsnwTs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# List of words\n",
        "words = ['texas', 'pizza', 'train', 'doctor', 'nurse', 'lion', 'tiger', 'airplane', 'helicopter', 'grape', 'mango', 'laptop', 'smartphone']\n",
        "\n",
        "# Get the word vectors for these words\n",
        "word_vectors = np.array([wv[word] for word in words])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(word_vectors)\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarity_matrix, xticklabels=words, yticklabels=words, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Cosine Similarity Heatmap')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iL1WgefnwTu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgmgCneb4_EJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}