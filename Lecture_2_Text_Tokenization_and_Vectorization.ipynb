{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAMIDSpiyalong/Gen-AI/blob/main/Lecture_2_Text_Tokenization_and_Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDqRHsHmYH_A"
      },
      "source": [
        "# Text Tokenziation and Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgoqZrZLYPlS"
      },
      "source": [
        "Text tokenization is a crucial step in NLP that involves reformatting a piece of text into smaller units called “tokens.” These tokens serve as the building blocks for text vectorization, which converts text into numerical representations (vectors) that machine learning models can work with. A good place to loop up popular tokenizors is https://tiktokenizer.vercel.app/. It is important to know that this tokenization process is not standarized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3LXiETfGIY"
      },
      "source": [
        "# Using Pretrained, Third-Party Vectorizors\n",
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58V-1VqHbQpI"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "\n",
        "\n",
        "# Print word vectors (embeddings) for a specific word\n",
        "print(wv.wv['pizza'])  # Vector for the word 'word2vec'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQkCrzSEbQpJ"
      },
      "source": [
        "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html. To save time and space, we'll limit ourselves to 200,000 word vectors for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jpuGamMbQpJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select some words to visualize\n",
        "words = ['king', 'queen', 'man', 'woman', 'apple', 'banana', 'car', 'train', 'computer']\n",
        "\n",
        "# Get the word vectors for these words\n",
        "word_vectors = [wv[word] for word in words]\n",
        "\n",
        "# Perform PCA to reduce dimensionality to 2D\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_2d = pca.fit_transform(word_vectors)\n",
        "\n",
        "# Plot the words in 2D space\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o')\n",
        "\n",
        "# Annotate each point with the corresponding word\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11hDkconbQpJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF2wzeFXnwTs"
      },
      "source": [
        "Many ways to calculate the cosine similarity between words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PzpqlNsnwTs"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(A, B):\n",
        "    dot_product = np.dot(A, B)\n",
        "    norm_A = np.linalg.norm(A)\n",
        "    norm_B = np.linalg.norm(B)\n",
        "    similarity = dot_product / (norm_A * norm_B)\n",
        "    return similarity\n",
        "\n",
        "cosine_similarity(pizza, train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toP3xADFnwTs"
      },
      "outputs": [],
      "source": [
        "want = wv['italy']\n",
        "\n",
        "cosine_similarity(pizza, want)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iL1WgefnwTu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CErnAts8490j"
      },
      "source": [
        "## Conclustion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypR4pnsp4_83"
      },
      "source": [
        "In this NLP lab, we delved into text tokenization and vectorization, starting with bag-of-words (BOW) representation for converting text into numerical forms using scikit-learn, then progressing to TF-IDF for assessing term importance in documents relative to a corpus. We utilized TF-IDF for text search and semantic analysis via cosine similarity metrics. Implementing a basic search engine using TF-IDF showcased practical application. Additionally, we explored pre-trained word vectors like Google News vectors, demonstrating how third-party resources can enhance NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgmgCneb4_EJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}